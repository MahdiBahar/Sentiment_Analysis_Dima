{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08883d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b4c23e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns= None\n",
    "pd.options.display.max_colwidth= None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202ea709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram_analysis.py\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from hazm import Normalizer, word_tokenize, Stemmer, Lemmatizer, stopwords_list\n",
    "\n",
    "from connect_to_database_func import connect_db\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Fetch comments from DB\n",
    "# ---------------------------\n",
    "def fetch_comments(sentiment=None, min_len=2, limit=None):\n",
    "    \"\"\"\n",
    "    Fetch comments (id, title, grade, description, sentiment_result) from DB.\n",
    "    Optionally filter by sentiment ('negative', 'positive', etc.) and non-empty text.\n",
    "    \"\"\"\n",
    "    conn = connect_db()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    base = \"\"\"\n",
    "        SELECT id, title, grade, description, COALESCE(sentiment_result, '') as sentiment_result\n",
    "        FROM comments\n",
    "        WHERE description IS NOT NULL\n",
    "          AND trim(description) <> ''\n",
    "    \"\"\"\n",
    "    args = []\n",
    "    if sentiment:\n",
    "        base += \" AND lower(sentiment_result) = lower(%s)\"\n",
    "        args.append(sentiment)\n",
    "    base += \" ORDER BY id ASC\"\n",
    "    if limit:\n",
    "        base += \" LIMIT %s\"\n",
    "        args.append(limit)\n",
    "\n",
    "    cur.execute(base, tuple(args))\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"id\", \"title\", \"grade\", \"description\", \"sentiment_result\"])\n",
    "    # drop very short strings\n",
    "    df = df[df[\"description\"].str.len() >= min_len].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf777bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e424ce1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>grade</th>\n",
       "      <th>description</th>\n",
       "      <th>sentiment_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>پرداخت قبض</td>\n",
       "      <td>3</td>\n",
       "      <td>جایی نداره که من بنویسم این موبایل به نام چه ک...</td>\n",
       "      <td>no sentiment expressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>پرداخت قبض</td>\n",
       "      <td>3</td>\n",
       "      <td>به من نگفت که شماره تلفن را باید با کد شهر وار...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>پرداخت قبض</td>\n",
       "      <td>3</td>\n",
       "      <td>قبض تلفن ثابت رو پرداخت کردم. به من نگفت با پی...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>پرداخت قبض</td>\n",
       "      <td>4</td>\n",
       "      <td>پس از پرداخت قبض و در حین باز شدن منوی تجربه، ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>سایر</td>\n",
       "      <td>1</td>\n",
       "      <td>منوی حالات نمایش بصورت دوحالت تاریک و روشن میب...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6640</th>\n",
       "      <td>14918</td>\n",
       "      <td>انتقال وجه</td>\n",
       "      <td>5</td>\n",
       "      <td>خیلی عالی بود\\n ازکار کردن بااپ لذت بردم</td>\n",
       "      <td>very positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6641</th>\n",
       "      <td>14927</td>\n",
       "      <td>خرید شارژ</td>\n",
       "      <td>5</td>\n",
       "      <td>عالی بود وباسرعت زیاد</td>\n",
       "      <td>very positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6642</th>\n",
       "      <td>14937</td>\n",
       "      <td>انتقال وجه</td>\n",
       "      <td>4</td>\n",
       "      <td>رسید پیچیده است میتواند مفهومی تر مینمال‌تر با...</td>\n",
       "      <td>no sentiment expressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6643</th>\n",
       "      <td>14940</td>\n",
       "      <td>انتقال وجه</td>\n",
       "      <td>5</td>\n",
       "      <td>عالی بود</td>\n",
       "      <td>very positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6644</th>\n",
       "      <td>14949</td>\n",
       "      <td>انتقال وجه</td>\n",
       "      <td>5</td>\n",
       "      <td>عالی</td>\n",
       "      <td>very positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6645 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       title  grade  \\\n",
       "0         1  پرداخت قبض      3   \n",
       "1         2  پرداخت قبض      3   \n",
       "2         3  پرداخت قبض      3   \n",
       "3         4  پرداخت قبض      4   \n",
       "4         5        سایر      1   \n",
       "...     ...         ...    ...   \n",
       "6640  14918  انتقال وجه      5   \n",
       "6641  14927   خرید شارژ      5   \n",
       "6642  14937  انتقال وجه      4   \n",
       "6643  14940  انتقال وجه      5   \n",
       "6644  14949  انتقال وجه      5   \n",
       "\n",
       "                                            description  \\\n",
       "0     جایی نداره که من بنویسم این موبایل به نام چه ک...   \n",
       "1     به من نگفت که شماره تلفن را باید با کد شهر وار...   \n",
       "2     قبض تلفن ثابت رو پرداخت کردم. به من نگفت با پی...   \n",
       "3     پس از پرداخت قبض و در حین باز شدن منوی تجربه، ...   \n",
       "4     منوی حالات نمایش بصورت دوحالت تاریک و روشن میب...   \n",
       "...                                                 ...   \n",
       "6640           خیلی عالی بود\\n ازکار کردن بااپ لذت بردم   \n",
       "6641                              عالی بود وباسرعت زیاد   \n",
       "6642  رسید پیچیده است میتواند مفهومی تر مینمال‌تر با...   \n",
       "6643                                           عالی بود   \n",
       "6644                                               عالی   \n",
       "\n",
       "            sentiment_result  \n",
       "0     no sentiment expressed  \n",
       "1                   negative  \n",
       "2                   negative  \n",
       "3                   negative  \n",
       "4                   negative  \n",
       "...                      ...  \n",
       "6640           very positive  \n",
       "6641           very positive  \n",
       "6642  no sentiment expressed  \n",
       "6643           very positive  \n",
       "6644           very positive  \n",
       "\n",
       "[6645 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# 2) Persian normalization/tokenizing\n",
    "# ------------------------------------\n",
    "_normalizer = Normalizer(persian_numbers=True, remove_diacritics=True)\n",
    "_stemmer = Stemmer()\n",
    "_lem = Lemmatizer()\n",
    "\n",
    "# Base stopwords (Hazm) + a few domain ones you might not want in n-grams\n",
    "BASE_STOPWORDS = set(stopwords_list()) | {\n",
    "    \"بانک\", \"ملت\", \"بانکملت\", \"دیما\", \"دیم\", \"اپ\", \"اپلیکیشن\", \"برنامه\",\n",
    "    \"سلام\", \"ممنون\", \"لطفا\", \"لطفاً\", \"خواهش\", \"خیلی\", \"کردم\", \"میشه\", \"میشود\",\n",
    "    # add more after inspecting top n-grams\n",
    "}\n",
    "\n",
    "_url_re = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "_handle_re = re.compile(r\"[@#]\\S+\")\n",
    "_num_re = re.compile(r\"\\d+\")\n",
    "_punct_re = re.compile(r\"[^\\w\\s‌]\")  # keep Persian ZWNJ (‌)\n",
    "\n",
    "def clean_and_tokenize(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    # normalize\n",
    "    t = _normalizer.normalize(text)\n",
    "\n",
    "    # remove URLs, handles, excessive numbers/punct\n",
    "    t = _url_re.sub(\" \", t)\n",
    "    t = _handle_re.sub(\" \", t)\n",
    "    t = _num_re.sub(\" \", t)\n",
    "    t = _punct_re.sub(\" \", t)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(t)\n",
    "\n",
    "    # lower, strip, remove stopwords/short tokens\n",
    "    cleaned = []\n",
    "    for tok in tokens:\n",
    "        tok = tok.strip().lower()\n",
    "        if len(tok) < 2:\n",
    "            continue\n",
    "        if tok in BASE_STOPWORDS:\n",
    "            continue\n",
    "        # optional lemmatize or stem (pick one; here lemmatize first)\n",
    "        tok = _lem.lemmatize(tok)\n",
    "        if tok in BASE_STOPWORDS or len(tok) < 2:\n",
    "            continue\n",
    "        cleaned.append(tok)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0448a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 3) Count top n-grams (bigrams/trigrams by default)\n",
    "# ---------------------------------------------------\n",
    "def top_ngrams(\n",
    "    texts,\n",
    "    ngram_range=(2, 3),\n",
    "    top_k=30,\n",
    "    min_df=5,\n",
    "    max_df=0.5,\n",
    "    max_features=20000\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a CountVectorizer over cleaned Persian tokens and return top n-grams with counts.\n",
    "    \"\"\"\n",
    "    # Important: we pass a custom tokenizer and disable token_pattern\n",
    "    vectorizer = CountVectorizer(\n",
    "        tokenizer=clean_and_tokenize,\n",
    "        preprocessor=lambda x: x,\n",
    "        token_pattern=None,\n",
    "        ngram_range=ngram_range,\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        max_features=max_features\n",
    "    )\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    counts = X.sum(axis=0).A1\n",
    "\n",
    "    df_counts = pd.DataFrame({\"ngram\": vocab, \"count\": counts})\n",
    "    # add n (length of ngram) for clarity\n",
    "    df_counts[\"n\"] = df_counts[\"ngram\"].str.count(\" \") + 1\n",
    "    df_counts = df_counts.sort_values(\"count\", ascending=False).head(top_k).reset_index(drop=True)\n",
    "    return df_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 4) Save results to DB (optional but handy)\n",
    "# -----------------------------------------\n",
    "def save_ngram_stats(df_counts, slice_name=\"all\"):\n",
    "    \"\"\"\n",
    "    Save n-gram stats into a table for dashboarding.\n",
    "    CREATE TABLE IF NOT EXISTS ngram_stats (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        slice_name TEXT,       -- e.g. 'all', 'negative', 'positive', 'payments'\n",
    "        n INT,\n",
    "        ngram TEXT,\n",
    "        count INT,\n",
    "        computed_at TIMESTAMP\n",
    "    );\n",
    "    \"\"\"\n",
    "    if df_counts.empty:\n",
    "        return\n",
    "\n",
    "    conn = connect_db()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # make sure the table exists (idempotent)\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ngram_stats (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            slice_name TEXT,\n",
    "            n INT,\n",
    "            ngram TEXT,\n",
    "            count INT,\n",
    "            computed_at TIMESTAMP\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    now = datetime.utcnow()\n",
    "    rows = [\n",
    "        (slice_name, int(row.n), str(row.ngram), int(row.count), now)\n",
    "        for _, row in df_counts.iterrows()\n",
    "    ]\n",
    "\n",
    "    cur.executemany(\"\"\"\n",
    "        INSERT INTO ngram_stats (slice_name, n, ngram, count, computed_at)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "    \"\"\", rows)\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8da434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# 5) Example runner (CLI)\n",
    "# ------------------------\n",
    "def main():\n",
    "    # A) ALL comments\n",
    "    df_all = fetch_comments()\n",
    "    df_top_all = top_ngrams(df_all[\"description\"].tolist(), ngram_range=(2,3), top_k=30, min_df=5)\n",
    "    print(\"\\nTop n-grams (ALL):\\n\", df_top_all.head(20))\n",
    "    save_ngram_stats(df_top_all, slice_name=\"all\")\n",
    "\n",
    "    # B) NEGATIVE comments only (pain points)\n",
    "    df_neg = fetch_comments(sentiment=\"negative\")\n",
    "    if not df_neg.empty:\n",
    "        df_top_neg = top_ngrams(df_neg[\"description\"].tolist(), ngram_range=(2,3), top_k=30, min_df=3)\n",
    "        print(\"\\nTop n-grams (NEGATIVE):\\n\", df_top_neg.head(20))\n",
    "        save_ngram_stats(df_top_neg, slice_name=\"negative\")\n",
    "\n",
    "    # C) POSITIVE (what users love)\n",
    "    df_pos = fetch_comments(sentiment=\"positive\")\n",
    "    if not df_pos.empty:\n",
    "        df_top_pos = top_ngrams(df_pos[\"description\"].tolist(), ngram_range=(2,3), top_k=30, min_df=3)\n",
    "        print(\"\\nTop n-grams (POSITIVE):\\n\", df_top_pos.head(20))\n",
    "        save_ngram_stats(df_top_pos, slice_name=\"positive\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
